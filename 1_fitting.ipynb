{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import EarlyStopping\n",
    "from models import RegressionNet, RegressionDropout, RegressionWithBatchNorm, CustomModel1, CustomModel2\n",
    "from generator import generate_data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 获取当前时间戳并生成唯一的log目录\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel1(\n",
       "  (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=6400, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\"\"\"提交到服务器前修改超参数\"\"\"\n",
    "sample_nums = 100\n",
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "patience =5\n",
    "\"\"\"\n",
    "构建网络 创建model时候指定参数\n",
    "选择损失函数和优化器\n",
    "\"\"\"\n",
    "input_size = 400\n",
    "hidden_size = 16\n",
    "output_size = 5\n",
    "model = CustomModel1()\n",
    "# 训练记录名修改\n",
    "# 格式 {模型名称}_{超参数}_{时间}\n",
    "# 不同模型的超参数可能不同\n",
    "writer = SummaryWriter(log_dir=f'./logs/{model.name}_{current_time}')\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.00001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "除非对数据集划分比例有要求否则无需更改\n",
    "\"\"\"\n",
    "x_data, y_data = generate_data(sample_nums, normal_distribution= False)\n",
    "input_data = y_data\n",
    "output_data = x_data\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(input_data, output_data, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with cpu\n",
      "Epoch 1/2, Training Loss : 1007.1515, Validation Loss: 567.1935\n",
      "Test Loss:567.4401848347982\n",
      "Epoch 2/2, Training Loss : 553.2293, Validation Loss: 508.2712\n",
      "Test Loss:530.1266231262207\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "除非网络对输入有特殊要求非则无需改动\n",
    "\"\"\"\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "stopped_epoch = None\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "print(f\"training with {device}\")\n",
    "save_path = f\"./weight/{model.name}\"\n",
    "os.makedirs(f\"{save_path}\", exist_ok=True)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.unsqueeze(1)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            batch_x = batch_x.unsqueeze(1)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            batch_x = batch_x.unsqueeze(1)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss : {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    print(f'Test Loss:{test_loss}')\n",
    "    writer.add_scalars('Loss', {'Train': train_loss, \n",
    "                            'Validation': val_loss, \n",
    "                            'Test': test_loss}, epoch)\n",
    "    writer.add_scalar('Learning Rate', current_lr, epoch)\n",
    "    \n",
    "    if epoch > 30:\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch:{epoch}\")\n",
    "            stopped_epoch = epoch\n",
    "            break\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        best_model_epoch = epoch\n",
    "        \n",
    "writer.close()\n",
    "\n",
    "model_save_name = f\"{model.name}_{current_time}\"\n",
    "\n",
    "\n",
    "torch.save(best_model_state, f'{save_path}/{model_save_name}_{best_model_epoch}.pth')\n",
    "if early_stopping.early_stop:\n",
    "    torch.save(model.state_dict(), f'{save_path}/{model_save_name}_{stopped_epoch}.pth')\n",
    "else:\n",
    "    torch.save(model.state_dict(), f'{save_path}/{model_save_name}_{num_epochs}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
